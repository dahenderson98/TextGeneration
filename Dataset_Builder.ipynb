{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "G00VkNzVEByL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXxlScMkuLcf"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data/data2'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "      os.makedirs(os.path.join(root))\n",
        "    datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "    extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "\n",
        "# A Tale of Two Cities\n",
        "url = 'https://www.gutenberg.org/files/98/98-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# A Christmas Carol\n",
        "url = 'https://www.gutenberg.org/files/24022/24022.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Our Mutual Friend\n",
        "url = 'https://www.gutenberg.org/files/883/883-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# David Copperfield\n",
        "url = 'https://www.gutenberg.org/files/766/766-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Oliver Twist v1\n",
        "url = 'https://www.gutenberg.org/files/47529/47529-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Oliver Twist v2\n",
        "url = 'https://www.gutenberg.org/files/47530/47530-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Oliver Twist v3\n",
        "url = 'https://www.gutenberg.org/files/47531/47531-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Hard Times\n",
        "url = 'https://www.gutenberg.org/files/786/786-0.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "# Bleak House\n",
        "url = 'https://www.gutenberg.org/files/1023/1023.zip'\n",
        "download_dataset(url)\n",
        "\n",
        "!ls ../data/data2/text\n",
        "\n",
        "# Move text files to working directory\n",
        "!mv ../data/data2/text/98-0.txt ./a_tale_of_two_cities.txt\n",
        "!mv ../data/data2/text/24022.txt ./a_christmas_carol.txt\n",
        "!mv ../data/data2/text/883-0.txt ./our_mutual_friend.txt\n",
        "!mv ../data/data2/text/766-0.txt ./david_copperfield.txt\n",
        "!mv ../data/data2/text/47529-0.txt ./oliver_twist_1.txt\n",
        "!mv ../data/data2/text/47530-0.txt ./oliver_twist_2.txt\n",
        "!mv ../data/data2/text/47531-0.txt ./oliver_twist_3.txt\n",
        "!mv ../data/data2/text/786-0.txt ./hard_times.txt\n",
        "!mv ../data/data2/text/1023.txt ./bleak_house.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all sentences and compile into a dataframe for each book\n",
        "# All dataframes have a single column 'lines' -- one sentence per line\n",
        "\n",
        "# T2C\n",
        "ttc_df = pd.DataFrame()\n",
        "f = open('./a_tale_of_two_cities.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "ttc_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# CC\n",
        "cc_df = pd.DataFrame()\n",
        "f = open('./a_christmas_carol.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "cc_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# OMF\n",
        "omf_df = pd.DataFrame()\n",
        "f = open('./our_mutual_friend.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "omf_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# DC\n",
        "dc_df = pd.DataFrame()\n",
        "f = open('./david_copperfield.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "dc_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# OT1\n",
        "ot1_df = pd.DataFrame()\n",
        "f = open('./oliver_twist_1.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "ot1_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# OT2\n",
        "ot2_df = pd.DataFrame()\n",
        "f = open('./oliver_twist_2.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "ot2_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# OT3\n",
        "ot3_df = pd.DataFrame()\n",
        "f = open('./oliver_twist_3.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "ot3_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# HT\n",
        "ht_df = pd.DataFrame()\n",
        "f = open('./hard_times.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "ht_df['lines'] = split_into_sentences(txt)\n",
        "\n",
        "# BH\n",
        "bh_df = pd.DataFrame()\n",
        "f = open('./bleak_house.txt')\n",
        "txt = f.read()\n",
        "f.close()\n",
        "bh_df['lines'] = split_into_sentences(txt)"
      ],
      "metadata": {
        "id": "mW2FvuLEx-VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove temp text files\n",
        "!rm ./a_tale_of_two_cities.txt\n",
        "!rm ./a_christmas_carol.txt\n",
        "!rm ./our_mutual_friend.txt\n",
        "!rm ./david_copperfield.txt\n",
        "!rm ./oliver_twist_1.txt\n",
        "!rm ./oliver_twist_2.txt\n",
        "!rm ./oliver_twist_3.txt\n",
        "!rm ./hard_times.txt\n",
        "!rm ./bleak_house.txt"
      ],
      "metadata": {
        "id": "oJw9NdtfJqNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all book dataframes into single dataframe\n",
        "df = pd.concat([ttc_df, cc_df, omf_df, dc_df, ot1_df, ot2_df, ot3_df, ht_df, bh_df], \n",
        "               axis = 0, ignore_index=True)"
      ],
      "metadata": {
        "id": "ez4nKqJUHUpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dataframe as a csv dataset\n",
        "df.to_csv('./charles_dickens.csv')"
      ],
      "metadata": {
        "id": "jqlsgGPl-cBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.lines[15000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrpj0vgrIqmt",
        "outputId": "01152ccf-1264-416c-db23-0e9a90bd00d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "’  ‘Don’t you fear ME no more, ma’am,’ said Betty; ‘I thought of it for good yesterday.\n"
          ]
        }
      ]
    }
  ]
}